{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        \n",
    "        tp = tf.reduce_sum(y_true * y_pred)\n",
    "        fp = tf.reduce_sum(y_pred) - tp\n",
    "        fn = tf.reduce_sum(y_true) - tp\n",
    "        \n",
    "        self.true_positives.assign_add(tp)\n",
    "        self.false_positives.assign_add(fp)\n",
    "        self.false_negatives.assign_add(fn)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.true_positives / (self.true_positives + self.false_positives + 1e-6)\n",
    "        recall = self.true_positives / (self.true_positives + self.false_negatives + 1e-6)\n",
    "        return 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(0)\n",
    "        self.false_positives.assign(0)\n",
    "        self.false_negatives.assign(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AdaptiveFocalLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, gamma, alpha, delta,\n",
    "                 reduction=tf.keras.losses.Reduction.AUTO, name='AdaptiveFocalLoss'):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        p_t = tf.clip_by_value(p_t, self.delta, 1.0 - self.delta)\n",
    "        alpha_factor = tf.where(tf.equal(y_true, 1), self.alpha, 1 - self.alpha)\n",
    "        focal_loss = -alpha_factor * tf.pow(1 - p_t, self.gamma) * tf.math.log(p_t)\n",
    "        return tf.reduce_mean(focal_loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"gamma\": self.gamma,\n",
    "            \"alpha\": self.alpha,\n",
    "            \"delta\": self.delta\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "class DynamicConv1D(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides=1, padding='same', num_experts=4, **kwargs):\n",
    "        super(DynamicConv1D, self).__init__(** kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.num_experts = num_experts\n",
    "        \n",
    "        self.experts = [\n",
    "            layers.Conv1D(filters, kernel_size, strides=strides, padding=padding,\n",
    "                         kernel_regularizer=l2(1e-4), use_bias=False)\n",
    "            for _ in range(num_experts)\n",
    "        ]\n",
    "        \n",
    "        self.gate = tf.keras.Sequential([\n",
    "            layers.GlobalAveragePooling1D(),\n",
    "            layers.Dense(num_experts, activation='softmax', kernel_regularizer=l2(1e-4))\n",
    "        ])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        gates = self.gate(inputs)  # [batch, num_experts]\n",
    "        gates = tf.reshape(gates, [batch_size, 1, 1, self.num_experts])\n",
    "        \n",
    "        expert_outputs = [tf.expand_dims(expert(inputs), axis=-1) for expert in self.experts]\n",
    "        expert_outputs = tf.concat(expert_outputs, axis=-1)\n",
    "        return tf.reduce_sum(expert_outputs * gates, axis=-1)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'filters': self.filters, 'kernel_size': self.kernel_size,\n",
    "            'strides': self.strides, 'padding': self.padding, 'num_experts': self.num_experts\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class Module(layers.Layer):\n",
    "    \"\"\"仅使用动态卷积的模块\"\"\"\n",
    "    def __init__(self, filters, kernel_size=3, strides=1, padding='same',\n",
    "                 num_experts=4, **kwargs):\n",
    "        super(Module, self).__init__(** kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.num_experts = num_experts\n",
    "        \n",
    "\n",
    "        self.dynamic_conv = tf.keras.Sequential([\n",
    "            DynamicConv1D(filters, kernel_size, strides, padding, num_experts),\n",
    "            layers.BatchNormalization(), layers.ReLU()\n",
    "        ])\n",
    "        \n",
    "        self.fusion = layers.Conv1D(filters, 1, kernel_regularizer=l2(1e-4))\n",
    "        self.bn_fusion = layers.BatchNormalization()\n",
    "        self.residual = layers.Conv1D(filters, 1, strides=strides, kernel_regularizer=l2(1e-4)) \\\n",
    "                        if strides != 1 else None\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        conv_out = self.dynamic_conv(inputs)\n",
    "\n",
    "        out = self.bn_fusion(self.fusion(conv_out)) \n",
    "        if self.residual:\n",
    "            out = layers.add([out, self.residual(inputs)])\n",
    "        return tf.nn.relu(out)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'filters': self.filters, 'kernel_size': self.kernel_size, 'strides': self.strides,\n",
    "            'padding': self.padding, 'num_experts': self.num_experts\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def save_results(indices, chr):\n",
    "    os.makedirs('results_25', exist_ok=True)\n",
    "    np.savetxt(f'results_25/{chr}_boundary.txt', indices, fmt='%d')\n",
    "\n",
    "\n",
    "def direct_predict_with_contiguous_processing(model, P_test, middle_row_indices_test, chr, threshold=0.5):\n",
    "    \"\"\"\n",
    "   Parameters:\n",
    "        model -- Trained machine learning model\n",
    "        P_test -- Test feature matrix (n_samples, n_features)\n",
    "        middle_row_indices_test -- Genomic position index array\n",
    "        y_test -- Test set labels (n_samples,)\n",
    "        chr -- Chromosome identifier\n",
    "        threshold -- Probability threshold (default 0.5)\n",
    "    \"\"\"\n",
    "  \n",
    "    y_pred_proba = model.predict(P_test).flatten()\n",
    "    prob_map = {idx: prob for idx, prob in zip(middle_row_indices_test, y_pred_proba)}\n",
    "    \n",
    "    selected = middle_row_indices_test[y_pred_proba > threshold]\n",
    "    sorted_indices = np.sort(np.unique(selected))\n",
    "\n",
    "    final_indices = []\n",
    "    current_segment = []\n",
    "\n",
    "    for idx in sorted_indices:\n",
    "        if not current_segment:\n",
    "            current_segment.append(idx)\n",
    "        else:\n",
    "            if idx == current_segment[-1] + 1:\n",
    "                current_segment.append(idx)\n",
    "            else:\n",
    "                if len(current_segment) >= 1:\n",
    "                    max_prob_idx = max(current_segment, key=lambda x: prob_map[x])\n",
    "                    final_indices.append(max_prob_idx)\n",
    "                current_segment = [idx]\n",
    "\n",
    "    if current_segment:\n",
    "        max_prob_idx = max(current_segment, key=lambda x: prob_map[x])\n",
    "        final_indices.append(max_prob_idx)\n",
    "    save_results(np.array(final_indices), chr)\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_node_degrees(H):\n",
    "    return np.sum(H, axis=1)\n",
    "def compute_hyperedge_degrees(H):\n",
    "    return np.sum(H, axis=0)\n",
    "\n",
    "def compute_first_order_transition_probabilities(H, node_degrees, hyperedge_degrees):\n",
    "    num_nodes, num_hyperedges = H.shape\n",
    "    P1 = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    for v in range(num_nodes):\n",
    "        for u in range(num_nodes):\n",
    "            if u == v:\n",
    "                continue\n",
    "\n",
    "            pi_uv = 0\n",
    "            for e in range(num_hyperedges):\n",
    "                if H[u, e] == 0 or H[v, e] == 0:  \n",
    "                    continue\n",
    "\n",
    "                h_ve = H[v, e] \n",
    "                h_ue = H[u, e] \n",
    "                d_v = node_degrees[v] \n",
    "                delta_e = hyperedge_degrees[e] \n",
    "                pi_uv += (h_ve * h_ue) / (d_v * delta_e)\n",
    "\n",
    "            P1[v, u] = pi_uv\n",
    "\n",
    "    return P1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "custom_objects = {\n",
    "    'Module': Module,\n",
    "    'DynamicConv1D': DynamicConv1D,\n",
    "    'F1Score': F1Score,\n",
    "    'AdaptiveFocalLoss': AdaptiveFocalLoss\n",
    "}\n",
    "\n",
    "model = load_model('HyperDeepTAD/model/best_model_seed_4200_lr_0.0003.h5', custom_objects=custom_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_files_to_arrays(filenames):\n",
    "    X_all_chr = []\n",
    "    middle_row_indices_all_chr = []\n",
    "    # y_all_chr = []\n",
    "    sample_counts = []\n",
    "\n",
    "    for filename in filenames:\n",
    "        count = 0\n",
    "        with open(filename, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    data = eval(line, {\"array\": np.array})\n",
    "                    X_all_chr.append(data[0])\n",
    "                    middle_row_indices_all_chr.append(data[1])\n",
    "                    # y_all_chr.append(data[2])\n",
    "                    count += 1\n",
    "        sample_counts.append(count)\n",
    "\n",
    "    return (np.array(X_all_chr),\n",
    "            np.array(middle_row_indices_all_chr),\n",
    "            # np.array(y_all_chr),\n",
    "            sample_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "dir1 = '/public_data/liukaihua/Article/model/model_data/'\n",
    "dir2 = '25_sub_matrix.txt'\n",
    "\n",
    "chr_list = ['chr20','chr21','chr22']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr20\n",
      "/public_data/liukaihua/Article/model/model_data/chr20_100_sub_matrix.txt\n",
      "['/public_data/liukaihua/Article/model/model_data/chr20_100_sub_matrix.txt']\n",
      "P_test shape: (635, 11, 11)\n",
      "20/20 [==============================] - 1s 14ms/step\n",
      "chr21\n",
      "/public_data/liukaihua/Article/model/model_data/chr21_100_sub_matrix.txt\n",
      "['/public_data/liukaihua/Article/model/model_data/chr21_100_sub_matrix.txt']\n",
      "P_test shape: (408, 11, 11)\n",
      "13/13 [==============================] - 0s 13ms/step\n",
      "chr22\n",
      "/public_data/liukaihua/Article/model/model_data/chr22_100_sub_matrix.txt\n",
      "['/public_data/liukaihua/Article/model/model_data/chr22_100_sub_matrix.txt']\n",
      "P_test shape: (393, 11, 11)\n",
      "13/13 [==============================] - 0s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "for chr in chr_list:\n",
    "    list1 = []\n",
    "    f1 = os.path.join(dir1, f\"{chr}_{dir2}\")\n",
    "    list1.append(f1)\n",
    "    print(chr)\n",
    "    print(f1)\n",
    "    print(list1)\n",
    "    X_test, middle_row_indices_test,sample_counts_test = process_files_to_arrays(list1)\n",
    "    P_test= []\n",
    "    for H in X_test:\n",
    "      \n",
    "        num_nodes, num_hyperedges = H.shape\n",
    "        \n",
    "        node_degrees = compute_node_degrees(H)\n",
    "        hyperedge_degrees = compute_hyperedge_degrees(H)\n",
    "       \n",
    "        P1 = compute_first_order_transition_probabilities(H, node_degrees, hyperedge_degrees)\n",
    "       \n",
    "        P_test.append(P1)\n",
    "    P_test = np.array(P_test)  \n",
    "    print(\"P_test shape:\", P_test.shape)\n",
    "    # direct_predict_simple(model,P_test,middle_row_indices_test,chr,threshold=0.5)\n",
    "    direct_predict_with_contiguous_processing(model,P_test,middle_row_indices_test,chr,threshold=0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
